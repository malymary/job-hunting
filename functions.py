import time
import json
import requests
from IPython.display import display, clear_output
import tqdm
import tqdm.notebook

import bs4
import pandas as pd
from collections import Counter
from wordcloud import WordCloud
from natasha import Doc, MorphVocab, NewsEmbedding, NewsMorphTagger, Segmenter
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import config

'''Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ JSON-Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ² Ñ€Ğ°Ğ±Ğ¾Ñ‡ÑƒÑ Ğ¿Ğ°Ğ¿ĞºÑƒ'''
'''Save JSON file to the working directory'''

def dump_json(obj, filename):
        with open(filename, 'w', encoding='UTF-8') as f: 
            json.dump(obj, f, ensure_ascii=False, indent=4)

            
'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨'''          
            
'''Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ±Ğ¾Ñ€Ğ° Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ñ ÑĞ°Ğ¹Ñ‚Ğ° hh.ru'''
'''Collect vacancies at hh.ru'''

def get_vacancies(page = config.PAGE,
    per_page = config.PER_PAGE,
    period = config.PERIOD,
    text = config.TEXT,
    experience = config.EXPERIENCE,
    employment= config.EMPLOYMENT,
    schedule = config.SCHEDULE,
    area= config.AREA,
    industry = config.INDUSTRY,
    salary = config.SALARY,
    only_with_salary = config.ONLY_WITH_SALARY):

    params = {
        'page': page,
        'per_page': per_page,
        'period': period,
        'text': text,
        'experience': experience,
        'employment': employment,
        'schedule': schedule,
        'area': area,
        'industry': industry,
        'salary': salary,
        'only_with_salary': only_with_salary,
        }
    
    res = requests.get('https://api.hh.ru/vacancies', params=params)
    if not res.ok:
        print('Error:', res)
    vacancies = res.json()['items']
    pages = res.json()['pages']

    for page in tqdm.trange(1, pages):
        params['page'] = page
        res = requests.get('https://api.hh.ru/vacancies', params=params)
        if res.ok:
            response_json = res.json()
            vacancies.extend(response_json['items'])
        else:
            print(res)
            
    # dump_json(vacancies, 'vacancies.json')
    return vacancies


'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨''' 

'''Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ„Ğ°Ğ¹Ğ»Ğ° Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ (Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ½Ğ¾ 100 Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ Ğ² Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ)'''
'''Get a new file with job descriptions (around 100 descriptions per min)'''

def get_full_descriptions(vacancies):
    vacancies_full = []
    for entry in tqdm.tqdm(vacancies):
        vacancy_id = entry['id']
        description = requests.get(f'https://api.hh.ru/vacancies/{vacancy_id}')
        vacancies_full.append(description.json())
        print(description.json())
        time.sleep(0.2)
        clear_output()
    dump_json(vacancies_full, 'vacancies_full.json')
    return vacancies_full


'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨''' 

'''Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ° c Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹ c Ğ“ÑƒĞ³Ğ» Ğ”Ğ¸ÑĞºĞ°'''
'''Download an existing file with job descriptions from Google Drive'''

def load_from_gdrive(file_id, filename):
    url = f'https://drive.google.com/uc?export=view&id={file_id}'
    res = requests.get(url)
    vacancies_full = res.json()
    dump_json(vacancies_full, filename)
    return vacancies_full


'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨''' 

'''Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒÑÑ‰ĞµĞ³Ğ¾ Ğ»Ğ¾ĞºĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ° c Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹'''
'''Open an existing local file with job descriptions'''

def load_from_device(file_path):
    with open(f'{file_path}', encoding='utf8') as f: vacancies_full = json.load(f)
    return vacancies_full


'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨''' 

'''Ğ¡Ğ¾ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ², Ğ¾Ñ‚ÑĞ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¿Ğ¾ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸'''
'''Make a list of key skills sorted by frequency'''

def sort_skills_by_freq(vacancies_full):
    all_skills = []
    for vacancy in vacancies_full:
        if vacancy['key_skills'] is None:
            continue
        else:
            for skill in vacancy['key_skills']:
                all_skills.append(skill['name'])
    frequencies = Counter(all_skills)

    freq_table = pd.DataFrame.from_dict(frequencies, orient='index', columns=['Frequency'])
    freq_table = freq_table.sort_values('Frequency', ascending=False)
    pd.set_option('display.max_rows', None)
    print(freq_table)
    
    return frequencies
    

'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨''' 

'''ĞÑ‚Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ¸Ñ‚ÑŒ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ²Ğ¾Ñ€Ğ´ĞºĞ»Ğ°ÑƒĞ´ Ñ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼Ğ¸ Ğ½Ğ°Ğ²Ñ‹ĞºĞ°Ğ¼Ğ¸'''
'''Display and save key skills as a wordcloud'''

def make_cloud(frequencies):
    print(f'Ğ¢Ğ¾Ğ¿ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ {config.TEXT}:')
    cloud = WordCloud(background_color='black', width=800, height=600, color_func=lambda *args, **kwargs: "white")
    my_image = cloud.generate_from_frequencies(frequencies=frequencies).to_image()
    display(my_image)
    my_image.save(f'top_skill_wordcloud.png')


'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨'''

def preprocess(text):
    """Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ²Ğ°Ñ€Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¸:
    '<p><strong>ĞšĞ¾Ğ³Ğ¾ Ğ¼Ñ‹ Ğ¸Ñ‰ĞµĞ¼:</strong><br/>Junior Backend Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ°, Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ğ² ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğµ.</p> <p><strong>'
     â†“ â†“ â†“
    'Ğ¸ÑĞºĞ°Ñ‚ÑŒ junior backend Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°'
    """
    parsed_html = bs4.BeautifulSoup(text)
    text = parsed_html.text  # ÑƒĞ´Ğ°Ğ»Ğ¸Ğ»Ğ¸ Ñ‚ÑĞ³Ğ¸

    morph_vocab = MorphVocab()
    segmenter = Segmenter()
    embedding = NewsEmbedding()
    morph_tagger = NewsMorphTagger(embedding)
    doc = Doc(text)
    doc.segment(segmenter)
    doc.tag_morph(morph_tagger)

    words = []

    for token in doc.tokens:
        # Ğ•ÑĞ»Ğ¸ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ€ĞµÑ‡Ğ¸ Ğ½Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ² ÑĞ¿Ğ¸ÑĞ¾Ğº: [Ğ·Ğ½Ğ°Ğº Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ğ¸, Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ³, ÑĞ¾ÑĞ·, Ğ¼ĞµÑÑ‚Ğ¾Ğ¸Ğ¼ĞµĞ½Ğ¸Ğµ], Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼:
        if token.pos not in ['PUNCT', 'ADP', 'CCONJ', 'PRON']:
            # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ Ğº Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğµ 'ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ¾Ğ²' -> 'ÑĞ¿Ğ¾ÑĞ¾Ğ±'
            token.lemmatize(morph_vocab)
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº
            words.append(token.lemma)

    # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ² Ğ¾Ğ´Ğ½Ñƒ ÑÑ‚Ñ€Ğ¾ĞºÑƒ Ñ‡ĞµÑ€ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»
    # ['Ğ¾Ğ±ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ', 'Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ',  'ĞºĞ¾Ğ´'] -> 'Ğ¾Ğ±ÑĞ·Ğ°Ğ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒ ĞºĞ¾Ğ´'
    line = ' '.join(words)

    return line


'''âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨ğŸ³âœ¨ğŸ¬âœ¨ğŸŸâœ¨ğŸ âœ¨'''

def preprocess_all(document_collection):
    """Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²ÑĞµÑ… Ğ²Ğ°ĞºĞ°Ğ½ÑĞ¸Ğ¹. ĞĞ° Ğ²Ñ…Ğ¾Ğ´ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸ÑĞ¼Ğ¸.
    Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ´Ğ¾ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ÑĞ°!
    """
    preprocessed = []
    for vacancy in tqdm.tqdm(vacancies_df['description']):
        preprocessed.append(preprocess(vacancy))

    dump_json(preprocessed, 'preprocessed.json')

    return preprocessed


def get_tf_idf_weights(preprocessed):
    """Ğ­Ñ‚Ğ° Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµÑ‚ Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ Ğ²Ğ¸Ğ´Ğ° 
    'Ğ¸ÑĞºĞ°Ñ‚ÑŒ junior backend Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸Ğº Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°'
    Ğ¸ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ¿Ğ¾ Ğ½Ğ¸Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ²ĞµÑĞ¾Ğ² ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²: 
    {'Ğ¸ÑĞºĞ°Ñ‚ÑŒ': 0.54, 'junior': 0.73, ...}
    """
    vectorizer = TfidfVectorizer(ngram_range=(1, 2))
    #  ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¾Ğ±ÑŠĞµĞºÑ‚ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ° (Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸, ĞºĞ¾Ğ´Ğ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ²Ğ¸Ğ´Ğµ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹ Ñ‡Ğ¸ÑĞµĞ»)
    vectorizer.fit(preprocessed)

    tf_idf_words = vectorizer.get_feature_names_out()
    tf_idf_table = vectorizer.transform(preprocessed).toarray()
    weights = tf_idf_table.sum(axis=0)
    indices_order = weights.argsort()[::-1]

    tf_idf_words[indices_order]

    frequencies = dict(zip(tf_idf_words, weights))

    return frequencies
